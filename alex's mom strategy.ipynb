{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f6269362",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from arch import arch_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aece536",
      "metadata": {},
      "source": [
        "step 1: implement volatility model (modified GARCH)  \n",
        "  \n",
        "$\\sigma_{n}^{2} = \\gamma V_{lt} +\\alpha u_{n-1}^{2} + \\beta\\sigma_{n-1}^{2}$  \n",
        "  \n",
        "where:  \n",
        "  \n",
        "$\\sigma_{n}$ is the volatility of the market variable for day $n$ (made at the end of day $n-1$). $\\sigma_{n}^{2}$ is the variance of the market variable for day $n$  \n",
        "$\\gamma$ is the weight assigned to $V_{lt}$  \n",
        "$V_{lt}$ is the long-run average variance rate  \n",
        "$\\alpha$ is the weight assigned to $u_{n-1}^{2}$   \n",
        "$u_{n-1}$ is the percentage change in the market variable between the end of day $n-2$ and the end of day $n-1$ (the most recent daily percentage change in the market variable). $u_{n-1}^{2}$ is the squared return of the market variable between the end of day $n-2$ and the end of day $n-1$  \n",
        "\n",
        "$u_{n-1} = \\frac{S_{n-1} - S_{n-2}}{S_{n-2}}$  \n",
        "\n",
        "$S_{n-1}$ is the value of the market variable at the end of day $n-1$  \n",
        "$S_{n-2}$ is the value of the market variable at the end of day $n-2$  \n",
        "\n",
        "$\\beta$ is the weight assigned to $\\sigma_{n-1}^{2}$  \n",
        "$\\sigma_{n-1}$ is the volatility of the market variable for day $n-1$ (made at the end of day $n-2$) . $\\sigma_{n-1}^{2}$ is the variance of the market variable for day $n-1$  \n",
        "  \n",
        "since the weights must sum to unity, it follows that  \n",
        "  \n",
        "$\\gamma+\\alpha+\\beta=1$  \n",
        "  \n",
        "setting $\\omega=\\gamma V_{lt}$, the model can also be written as  \n",
        "  \n",
        "$\\sigma_{n}^{2} = \\omega +\\alpha u_{n-1}^{2} + \\beta\\sigma_{n-1}^{2}$  \n",
        "  \n",
        "we include one lag of an asymmetric shock which transforms a GARCH model into a GJR-GARCH model with variance dynamics given by  \n",
        "  \n",
        "$\\sigma^2_n   =  \\omega + \\alpha u_{n-1}^2 + \\gamma u_{n-1}^2 I_{[u_{n-1}<0]}+ \\beta \\sigma_{n-1}^2$\n",
        "\n",
        "where $I$ an indicator function that takes the value 1 when its argument is true. the log likelihood improves substantially with the introduction of an asymmetric term, and the parameter estimate is highly significant.  \n",
        "\n",
        "to improve the fit a little more, we model the volatility using absolute values rather than a variance process that evolves in squares. this process, known as a TARCH/ZARCH model is given by  \n",
        "  \n",
        "$\\sigma_n  =  \\omega + \\alpha \\left|u_{n-1}\\right| + \\gamma \\left|u_{n-1}\\right| I_{[u_{n-1}<0]}+ \\beta \\sigma_{n-1}$\n",
        "\n",
        "one final adjustment, financial returns are often heavy tailed, and a Student’s T distribution is a simple method to capture this feature. The call to arch changes the distribution from a Normal to a Students’s T."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07bb969",
      "metadata": {},
      "outputs": [],
      "source": [
        "def volatility(df):\n",
        "    # df: dataframe of price data. uses GJR-GARCH/TARCH (power=1) + t distribution.\n",
        "    df = df.copy()\n",
        "    returns = 100 * df[\"Close\"].pct_change().dropna()\n",
        "    df = df.iloc[1:].copy()\n",
        "\n",
        "    am = arch_model(returns, p=1, o=1, q=1, power=1.0, dist=\"studentst\")\n",
        "    result = am.fit()\n",
        "    df.loc[:, \"Volatility\"] = result._volatility\n",
        "    df.loc[:, \"Rolling Volatility\"] = df[\"Volatility\"].rolling(window=10).mean()\n",
        "\n",
        "    calculated = df[\"Volatility\"].mean()\n",
        "    expected = np.var(returns)\n",
        "    print(f\"percent error in volatility calculation: {(np.abs(calculated - expected) / expected) * 100}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c10f669",
      "metadata": {},
      "source": [
        "step 2: momentum  \n",
        "don't have much to say about this it's pretty straightforward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bd8404",
      "metadata": {},
      "outputs": [],
      "source": [
        "def absolute_momentum(df):\n",
        "    # 4 month momentum\n",
        "    df = df.copy()\n",
        "    df['Momentum'] = (df[\"Close\"] / df[\"Close\"].shift(84)) - 1\n",
        "    df.dropna()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98646a32",
      "metadata": {},
      "source": [
        "step 3: ATR Trend/Breakout system  \n",
        "produces a signal based on bands. if a given's day high is higher than the upper band, the following day the model will go long (signal = 2). if a given day's low is lower than the lower band, the following day the model will go neutral/short (signal = -2). the bands are defined by the 42-day rolling average true range, with the true range being given by  \n",
        "  \n",
        "$TR = max\\left[(H-L), \\left|H-C_{n-1}\\right|, \\left|L-C_{n-1}\\right|\\right]$  \n",
        "  \n",
        "where $H$ is the day's high, $L$ is the day's low, and $C_{n-1}$ is the previous close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9fed82",
      "metadata": {},
      "outputs": [],
      "source": [
        "def atr_trend_breakout(df):\n",
        "    df = df.copy()\n",
        "    high = df[\"High\"]\n",
        "    low = df[\"Low\"]\n",
        "    close = df[\"Close\"]\n",
        "    tr = pd.concat([high - low, (high - close.shift(1)).abs(), (low - close.shift(1)).abs()], axis=1).max(axis=1)\n",
        "    atr = tr.rolling(42).mean()\n",
        "    upper = high.rolling(42).max() + atr  # upper = session highs + ATR\n",
        "    lower = low.rolling(42).min() - atr\n",
        "    signal = pd.Series(0.0, index=df.index)\n",
        "    long_mask = high > upper\n",
        "    short_mask = low < lower\n",
        "    signal.loc[long_mask] = 2\n",
        "    signal.loc[short_mask] = -2\n",
        "    df['ATR Trend/Breakout'] = signal.replace(0, np.nan).ffill().fillna(0)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d678452",
      "metadata": {},
      "source": [
        "step 4: average relative correlations  \n",
        "4 months average correlation across the ETFs on daily returns. requires proper merged dataframe but thats for me to figure out later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a845a08",
      "metadata": {},
      "outputs": [],
      "source": [
        "def correlation(returns_df):\n",
        "    tickers = returns_df.columns.tolist()\n",
        "    out = pd.DataFrame(index=returns_df.index, columns=tickers, dtype=float)\n",
        "    for i in range(84, len(returns_df) + 1):\n",
        "        chunk = returns_df.iloc[i - 84:i]\n",
        "        corr = chunk.corr()\n",
        "        idx = returns_df.index[i - 1]\n",
        "        for col in tickers:\n",
        "            others = [c for c in tickers if c != col]\n",
        "            if others:\n",
        "                out.loc[idx, col] = corr.loc[col, others].mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc70397e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ranks_for_raam(momentum_series, vol_series, corr_series):\n",
        "    \"\"\"\n",
        "    Rank 1..N with higher = better. M: higher momentum = higher rank. V,C: lower vol/corr = higher rank.\n",
        "    Returns (rank_m, rank_v, rank_c) as Series.\n",
        "    \"\"\"\n",
        "    from scipy.stats import rankdata\n",
        "    # Rank(M): 11 = best (highest momentum) -> rankdata(-M) gives 1 to best, so we want 1..11 with 11=best -> 12 - rankdata(-M) no: rankdata(-M) gives 1 to max M. So rankdata(-M) is 1 for best. Paper: \"ranked 1 to 11 in ascending order of M\" so lowest M gets 1. So rankdata(M) gives 1 to lowest. So Rank(M)=rankdata(M) means lowest M gets 1. But \"greater momentum = greater rank\" so we need Rank(M) = rankdata(-momentum): 1 for lowest M, 11 for highest M. So 11 = best. Good.\n",
        "    rank_m = pd.Series(rankdata(-momentum_series), index=momentum_series.index)\n",
        "    # Rank(V): lower vol = higher rank. rankdata(vol) gives 1 to lowest vol. So 1=best. We want 11=best so Rank(V) = 12 - rankdata(vol).\n",
        "    n = len(vol_series)\n",
        "    rank_v = pd.Series(n + 1 - rankdata(vol_series), index=vol_series.index)\n",
        "    rank_c = pd.Series(n + 1 - rankdata(corr_series), index=corr_series.index)\n",
        "    return rank_m, rank_v, rank_c\n",
        "\n",
        "def ranked_allocation(rank_m, rank_v, rank_c, w_m=1/3, w_v=1/3, w_c=1/3, top_n=5, cash_ticker=\"SHY\"):\n",
        "    \"\"\"\n",
        "    Total Rank = wM*(N+1-Rank(M)) + wV*(N+1-Rank(V)) + wC*(N+1-Rank(C)) so lower total = better.\n",
        "    Select top_n assets with lowest Total Rank. Exclude cash_ticker from selection.\n",
        "    \"\"\"\n",
        "    tickers = [x for x in rank_m.index if x != cash_ticker]\n",
        "    n = len(tickers)\n",
        "    total = w_m * (n + 1 - rank_m.reindex(tickers).fillna(0))\n",
        "    total += w_v * (n + 1 - rank_v.reindex(tickers).fillna(0))\n",
        "    total += w_c * (n + 1 - rank_c.reindex(tickers).fillna(0))\n",
        "    top = total.nsmallest(top_n).index.tolist()\n",
        "    return top\n",
        "\n",
        "def raam_weights(momentum_series, rank_m, rank_v, rank_c, w_m=1/3, w_v=1/3, w_c=1/3, top_n=5, cash_ticker=\"SHY\"):\n",
        "    \"\"\"\n",
        "    RAAM allocation: top 5 by total rank; if momentum < 0 replace with cash.\n",
        "    momentum_series: last-day momentum for each ticker (e.g. at month-end).\n",
        "    Returns dict ticker -> weight (sum 1.0).\n",
        "    \"\"\"\n",
        "    top = ranked_allocation(rank_m, rank_v, rank_c, w_m, w_v, w_c, top_n, cash_ticker)\n",
        "    weight_per = 1.0 / top_n\n",
        "    weights = {}\n",
        "    cash_weight = 0.0\n",
        "    for t in top:\n",
        "        if t in momentum_series and momentum_series[t] < 0:\n",
        "            cash_weight += weight_per\n",
        "        else:\n",
        "            weights[t] = weight_per\n",
        "    if cash_weight > 0:\n",
        "        weights[cash_ticker] = weights.get(cash_ticker, 0) + cash_weight\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd9254a",
      "metadata": {},
      "source": [
        "tickers in our portfolio:\n",
        "**Brazilian Equities:**  \n",
        "EWZ  \n",
        "FLBR  \n",
        "EWZS  \n",
        "  \n",
        "**International Equities:**  \n",
        "EFA  \n",
        "EEM  \n",
        "  \n",
        "**Brazilian Real Estate:**  \n",
        "HGBS11  \n",
        "BTLG11  \n",
        "  \n",
        "****Brazilian Natural Resources – Commodities:**\n",
        "BIAU39\n",
        "BSLV39\n",
        "BCOM39\n",
        "\n",
        "Brazilian Bonds:\n",
        "BLTN\n",
        "VWOB (Emerging Markets)\n",
        "\n",
        "International Bonds:\n",
        "IGOV\n",
        "\n",
        "Cash:\n",
        "SHY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31772911",
      "metadata": {},
      "outputs": [],
      "source": [
        "TICKERS = [\"EWZ\", \"FLBR\", \"EWZS\", \"EFA\", \"EEM\", \"HGBS11\", \"BTLG11\", \"BIAU39\", \"BSLV39\", \"BCOM39\", \"BLTN\", \"VWOB\", \"IGOV\", \"SHY\"]\n",
        "RISKY_TICKERS = [t for t in SEVEN_TWELVE_TICKERS if t != \"SHY\"]\n",
        "CASH_TICKER = \"SHY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8214499",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_7twelve_data(start=\"2005-01-01\", end=\"2025-01-01\"):\n",
        "    \"\"\"Download OHLC for 7Twelve tickers and align to common index.\"\"\"\n",
        "    data = {}\n",
        "    for t in SEVEN_TWELVE_TICKERS:\n",
        "        try:\n",
        "            d = yf.download(t, start=start, end=end, progress=False, auto_adjust=True)\n",
        "            if isinstance(d.columns, pd.MultiIndex):\n",
        "                d.columns = d.columns.droplevel(\"Ticker\")\n",
        "            if \"Close\" in d.columns and len(d) > 100:\n",
        "                data[t] = d[[\"Open\", \"High\", \"Low\", \"Close\"]].copy()\n",
        "        except Exception as e:\n",
        "            print(t, e)\n",
        "    # Align to common dates\n",
        "    common = None\n",
        "    for t, df in data.items():\n",
        "        idx = df.index\n",
        "        common = idx if common is None else common.intersection(idx)\n",
        "    for t in list(data.keys()):\n",
        "        data[t] = data[t].loc[common].ffill().bfill()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf7e148",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_raam_panel(data_dict, verbose=True):\n",
        "    \"\"\"\n",
        "    Build panel of M, V (10d smoothed vol), C for each ticker.\n",
        "    data_dict: ticker -> OHLC DataFrame. Uses pipeline's volatility model (GARCH) per asset.\n",
        "    \"\"\"\n",
        "    returns_dict = {}\n",
        "    vol_dict = {}\n",
        "    mom_dict = {}\n",
        "    for t, df in data_dict.items():\n",
        "        if df is None or len(df) < 252:\n",
        "            continue\n",
        "        if verbose:\n",
        "            print(f\"Fitting volatility for {t} ...\")\n",
        "        try:\n",
        "            vdf = volatility(df)\n",
        "            returns_dict[t] = vdf[\"Close\"].pct_change()\n",
        "            vol_dict[t] = vdf[\"Rolling Volatility\"]\n",
        "            mom_dict[t] = absolute_momentum(vdf)\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(t, e)\n",
        "    # Align\n",
        "    common = None\n",
        "    for t in returns_dict:\n",
        "        idx = returns_dict[t].dropna().index\n",
        "        common = idx if common is None else common.intersection(idx)\n",
        "    returns_df = pd.DataFrame({t: returns_dict[t].reindex(common).ffill().bfill() for t in returns_dict})\n",
        "    vol_df = pd.DataFrame({t: vol_dict[t].reindex(common).ffill().bfill() for t in vol_dict})\n",
        "    mom_df = pd.DataFrame({t: mom_dict[t].reindex(common).ffill().bfill() for t in mom_dict})\n",
        "    if verbose:\n",
        "        print(\"Computing average relative correlation ...\")\n",
        "    corr_df = average_relative_correlation(returns_df)\n",
        "    return returns_df, vol_df, mom_df, corr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69af0f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def raam_backtest(returns_df, vol_df, mom_df, corr_df, w_m=1/3, w_v=1/3, w_c=1/3, top_n=5, cash_ticker=None):\n",
        "    \"\"\"\n",
        "    Monthly backtest: at each month-end compute RAAM weights, hold next month.\n",
        "    Returns (monthly_returns_series, cumulative_series).\n",
        "    \"\"\"\n",
        "    from scipy.stats import rankdata\n",
        "    cash_ticker = cash_ticker or CASH_TICKER\n",
        "    risky = [c for c in returns_df.columns if c != cash_ticker]\n",
        "    month_ends = returns_df.resample(\"ME\").last().index\n",
        "    monthly_ret = returns_df.resample(\"ME\").apply(lambda x: (1 + x).prod() - 1)\n",
        "    strategy_ret = []\n",
        "    dates = []\n",
        "    for i, me in enumerate(month_ends):\n",
        "        if me not in vol_df.index or me not in mom_df.index:\n",
        "            continue\n",
        "        v = vol_df.loc[me].reindex(risky).dropna()\n",
        "        m = mom_df.loc[me].reindex(risky).dropna()\n",
        "        c = corr_df.loc[me].reindex(risky).dropna()\n",
        "        common = v.index.intersection(m.index).intersection(c.index).tolist()\n",
        "        if len(common) < 5:\n",
        "            continue\n",
        "        n_risky = len(common)\n",
        "        rank_m = pd.Series(rankdata(-m[common]), index=common)\n",
        "        rank_v = pd.Series(n_risky + 1 - rankdata(v[common]), index=common)\n",
        "        rank_c = pd.Series(n_risky + 1 - rankdata(c[common]), index=common)\n",
        "        top = ranked_allocation(rank_m, rank_v, rank_c, w_m, w_v, w_c, top_n, cash_ticker)\n",
        "        weight_per = 1.0 / top_n\n",
        "        weights = {}\n",
        "        cash_w = 0.0\n",
        "        for t in top:\n",
        "            if m.get(t, 0) < 0:\n",
        "                cash_w += weight_per\n",
        "            else:\n",
        "                weights[t] = weight_per\n",
        "        if cash_w > 0:\n",
        "            weights[cash_ticker] = weights.get(cash_ticker, 0) + cash_w\n",
        "        next_me = month_ends[month_ends > me].min() if (month_ends > me).any() else None\n",
        "        if next_me is None:\n",
        "            break\n",
        "        ret_next = 0.0\n",
        "        for t, w in weights.items():\n",
        "            if t in monthly_ret.columns and next_me in monthly_ret.index:\n",
        "                r = monthly_ret.loc[next_me, t]\n",
        "                ret_next += w * (r if pd.notna(r) else 0)\n",
        "        strategy_ret.append(ret_next)\n",
        "        dates.append(next_me)\n",
        "    strat_series = pd.Series(strategy_ret, index=pd.DatetimeIndex(dates)).dropna()\n",
        "    cum = (1 + strat_series).cumprod()\n",
        "    return strat_series, cum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd129c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run RAAM strategy (uses pipeline volatility model)\n",
        "# Use shorter period and fewer tickers for a quicker run; expand as needed.\n",
        "START, END = \"2010-01-01\", \"2024-12-01\"\n",
        "tickers_subset = [\"SPY\", \"EFA\", \"VNQ\", \"DBC\", \"AGG\", \"BWX\", \"VGK\", \"VWO\", \"TIP\", \"LQD\", \"SHY\"]  # 11 assets\n",
        "\n",
        "data = load_7twelve_data(start=START, end=END)\n",
        "data = {t: data[t] for t in tickers_subset if t in data and data[t] is not None}\n",
        "returns_df, vol_df, mom_df, corr_df = build_raam_panel(data, verbose=True)\n",
        "# Backtest\n",
        "monthly_ret, cum = raam_backtest(returns_df, vol_df, mom_df, corr_df)\n",
        "print(\"RAAM monthly returns (last 12):\", monthly_ret.tail(12))\n",
        "print(\"Cumulative return:\", cum.iloc[-1] - 1 if len(cum) else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9470a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot RAAM vs buy-and-hold SPY (monthly)\n",
        "if len(cum) > 0 and \"SPY\" in returns_df.columns:\n",
        "    spy_monthly = (1 + returns_df[\"SPY\"]).resample(\"ME\").prod() - 1\n",
        "    spy_cum = (1 + spy_monthly).cumprod()\n",
        "    ax = cum.reindex(spy_cum.index).ffill().plot(label=\"RAAM\", figsize=(10, 4))\n",
        "    spy_cum.reindex(cum.index).ffill().plot(ax=ax, label=\"SPY\")\n",
        "    plt.legend()\n",
        "    plt.title(\"RAAM (2018 Dow Award strategy) vs SPY\")\n",
        "    plt.ylabel(\"Cumulative return\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f2c25e",
      "metadata": {},
      "source": [
        "TODO:\n",
        "implement momentum\n",
        "implement ATR trend / breakout system\n",
        "implement average relative correlations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
